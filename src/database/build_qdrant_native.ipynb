{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "486e0f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##%% [markdown]\n",
    "# # Qdrant Ingestion & Indexing Pipeline\n",
    "#\n",
    "# **What this notebook does**:\n",
    "# 1. Creates a Qdrant collection with 384‚Äëdim vectors and an HNSW graph configured for fast search  \n",
    "# 2. Registers payload indexes on your metadata fields  \n",
    "# 3. Embeds and ingests your dataset of 500 startups from the CSV  \n",
    "# 4. Polls the collection until the HNSW index is fully built  \n",
    "# 5. Demonstrates vector similarity and metadata filtering  \n",
    "#  \n",
    "# **Key parameters**:\n",
    "# - **Vector dim**: 384 (MiniLM)  \n",
    "# - **HNSW**: `m=16`, `ef_construct=100`  \n",
    "# - **Metadata indexes**: all fields in `PAYLOAD_SCHEMA`  \n",
    "#  \n",
    "# This notebook is meant to serve as both runnable code and living documentation.\n",
    "\n",
    "##%% [markdown]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7756937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SRC Path: /home/prashant-agrawal/projects/company_talk2data/src\n"
     ]
    }
   ],
   "source": [
    "# %% üìù Souring Path\n",
    "import sys, os\n",
    "SRC_PATH = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"src\"))\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH)\n",
    "print(f\"‚úÖ SRC Path: {SRC_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f56eb004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Base Dir: /home/prashant-agrawal/projects/company_talk2data/src\n",
      "üìå CSV Path: /home/prashant-agrawal/projects/company_talk2data/src/Data/Enriched_Indian_Startup_Dataset.csv\n",
      "üìå Qdrant Local Path: /home/prashant-agrawal/projects/company_talk2data/src/database/qdrant_store_local_db/collection\n",
      "üìå Collection Name: indian_startups\n",
      "üìå Schema Path: /home/prashant-agrawal/projects/company_talk2data/src/schema/payload_schema.json\n"
     ]
    }
   ],
   "source": [
    "# Python imports & libraries\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "#from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from qdrant_client.http.models import Range\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance, PayloadSchemaType, FieldCondition, MatchValue, Filter,HnswConfigDiff\n",
    "from typing import Dict, Union, Any\n",
    "from langchain_together.embeddings import TogetherEmbeddings\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "\n",
    "# üöÄ Import your utility loaders\n",
    "from utils.qdrant_client_loader import get_qdrant_collection_name\n",
    "from utils.path_config import get_base_dir, get_data_path, get_qdrant_store_path, get_schema_path\n",
    "\n",
    "# %% üìÅ Paths\n",
    "BASE_DIR = get_base_dir()\n",
    "DATA_PATH = get_data_path()\n",
    "SCHEMA_OUTPUT_PATH = get_schema_path()\n",
    "qdrant_store_path = get_qdrant_store_path()\n",
    "COLLECTION_NAME = get_qdrant_collection_name()\n",
    "\n",
    "print(f\"üìå Base Dir: {BASE_DIR}\")\n",
    "print(f\"üìå CSV Path: {DATA_PATH}\")\n",
    "print(f\"üìå Qdrant Local Path: {qdrant_store_path}\")\n",
    "print(f\"üìå Collection Name: {COLLECTION_NAME}\")\n",
    "print(f\"üìå Schema Path: {SCHEMA_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b5a09e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utility: Normalization ---\n",
    "def normalize_field_name(field: str) -> str:\n",
    "    return (\n",
    "        field.strip().lower()\n",
    "        .replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        .replace(\"/\", \"_\")\n",
    "    )\n",
    "\n",
    "def normalize_field_value(value) -> str:\n",
    "    return str(value).strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e2015f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install sentence-transformers\n",
    "#%pip install -U langchain-huggingface\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "#embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2378bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load & Process Data ---\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df = df.dropna(subset=[\"Company Description (Long)\"]).reset_index(drop=True)\n",
    "\n",
    "def build_points(df):\n",
    "    for idx, row in df.iterrows():\n",
    "        metadata = {\n",
    "            normalize_field_name(str(k)): normalize_field_value(v)\n",
    "            for k, v in row.items() if pd.notna(v)\n",
    "        }\n",
    "        # Use only main description as page_content\n",
    "        content = str(row[\"Company Description (Long)\"]) if \"Company Description (Long)\" in row else \"\"\n",
    "        yield {\n",
    "            \"id\": int(idx),\n",
    "            \"vector\": embeddings_model.embed_query(content),\n",
    "            \"payload\": metadata\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4335c26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SRC Path: /home/prashant-agrawal/projects/company_talk2data/src\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 2. Qdrant Setup ---\n",
    "from schema.qdrant_schema import PAYLOAD_SCHEMA\n",
    "from qdrant_client.http.models import HnswConfig\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "if client.collection_exists(COLLECTION_NAME):\n",
    "    client.delete_collection(collection_name=COLLECTION_NAME)\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4378b30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from qdrant_client.http.models import PayloadSchemaType\n",
    "from schema.qdrant_schema import PAYLOAD_SCHEMA\n",
    "\n",
    "for field, schema in PAYLOAD_SCHEMA.items():\n",
    "    # Normalize names (snake_case, all lowercase)\n",
    "    if isinstance(schema, dict) and \"type\" in schema:\n",
    "        field_schema = schema[\"type\"]\n",
    "    else:\n",
    "        field_schema = schema\n",
    "    client.create_payload_index(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        field_name=field,\n",
    "        field_schema=field_schema\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4f1db35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ingested 500 points into indian_startups.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Upload Data ---\n",
    "points = list(build_points(df))\n",
    "client.upsert(collection_name=COLLECTION_NAME, points=points)\n",
    "print(f\"‚úÖ Ingested {len(points)} points into {COLLECTION_NAME}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bfb16148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n",
      "Indexed 0/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mindexed_vectors_count \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m info\u001b[38;5;241m.\u001b[39mpoints_count:\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ HNSW graph now built and ready!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import (\n",
    "    VectorParams, Distance,\n",
    "    HnswConfigDiff, OptimizersConfig\n",
    ")\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "if client.collection_exists(COLLECTION_NAME):\n",
    "    client.delete_collection(COLLECTION_NAME)\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(\n",
    "        size=384, distance=Distance.COSINE,\n",
    "        hnsw_config=HnswConfigDiff(\n",
    "            m=16,\n",
    "            ef_construct=100,\n",
    "            full_scan_threshold=10     # minimum allowed value is 10\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# upsert your 500 points\n",
    "points = list(build_points(df))\n",
    "client.upsert(collection_name=COLLECTION_NAME, points=points)\n",
    "\n",
    "# poll until all are indexed\n",
    "import time\n",
    "while True:\n",
    "    info = client.get_collection(COLLECTION_NAME)\n",
    "    print(f\"Indexed {info.indexed_vectors_count}/{info.points_count}\")\n",
    "    if info.indexed_vectors_count >= info.points_count:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "print(\"‚úÖ HNSW graph now built and ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "04bcbb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) **Explicitly build the HNSW graph**\n",
    "client.update_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    hnsw_config=HnswConfigDiff(\n",
    "        m=16,\n",
    "        ef_construct=100,\n",
    "        full_scan_threshold=100\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5cc2ff96",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: http://localhost:6333/collections/indian_startups/index/hnsw",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 18\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# ‚Äî after your upsert(‚Ä¶) call ‚Äî\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Trigger the HNSW index build via the official REST path\u001b[39;00m\n\u001b[1;32m      7\u001b[0m resp \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mput(\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:6333/collections/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCOLLECTION_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/index/hnsw\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     json\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     },\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m \u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîç HNSW index build started:\u001b[39m\u001b[38;5;124m\"\u001b[39m, resp\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m~/projects/netflix_talk2data/venv/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: http://localhost:6333/collections/indian_startups/index/hnsw"
     ]
    }
   ],
   "source": [
    "import requests, time\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# ‚Äî after your upsert(‚Ä¶) call ‚Äî\n",
    "#\n",
    "# Trigger the HNSW index build via the official REST path\n",
    "resp = requests.put(\n",
    "    f\"http://localhost:6333/collections/{COLLECTION_NAME}/index/hnsw\",\n",
    "    json={\n",
    "      \"field_name\": \"vector\",\n",
    "      \"hnsw_config\": {\n",
    "         \"m\": 16,\n",
    "         \"ef_construct\": 100,\n",
    "         \"full_scan_threshold\": 100\n",
    "      }\n",
    "    },\n",
    ")\n",
    "resp.raise_for_status()\n",
    "print(\"üîç HNSW index build started:\", resp.json())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9009510",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) Poll until it's done\n",
    "import time\n",
    "while True:\n",
    "    info = client.get_collection(COLLECTION_NAME)\n",
    "    print(f\"Indexed {info.indexed_vectors_count}/{info.points_count}\")\n",
    "    if info.indexed_vectors_count >= info.points_count:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "print(\"‚úÖ HNSW graph fully built‚Äîready for low‚Äëlatency searches!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba83a0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector config:       size=384 distance=<Distance.COSINE: 'Cosine'> hnsw_config=HnswConfigDiff(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=None, on_disk=None, payload_m=None) quantization_config=None on_disk=None datatype=None multivector_config=None\n",
      "Total points:        500\n",
      "Indexed vectors:     0\n",
      "HNSW parameters M:   16\n",
      "HNSW ef_construct:   100\n",
      "HNSW built segments: 8\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "info = client.get_collection(collection_name=COLLECTION_NAME)\n",
    "\n",
    "print(\"Vector config:      \", info.config.params.vectors)\n",
    "print(\"Total points:       \", info.points_count)\n",
    "print(\"Indexed vectors:    \", info.indexed_vectors_count)\n",
    "print(\"HNSW parameters M:  \", info.config.hnsw_config.m)\n",
    "print(\"HNSW ef_construct:  \", info.config.hnsw_config.ef_construct)\n",
    "print(\"HNSW built segments:\", info.segments_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43197aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Indexed 0/500\n",
      "üîÑ Indexed 0/500\n",
      "üîÑ Indexed 0/500\n",
      "üîÑ Indexed 0/500\n",
      "üîÑ Indexed 0/500\n",
      "üîÑ Indexed 0/500\n",
      "üîÑ Indexed 0/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m total:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ HNSW graph fully built!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ## 5. Verify HNSW Index Build\n",
    "\n",
    "# %%\n",
    "import time\n",
    "while True:\n",
    "    info = client.get_collection(collection_name=COLLECTION_NAME)\n",
    "    done, total = info.indexed_vectors_count, info.points_count\n",
    "    print(f\"üîÑ Indexed {done}/{total}\")\n",
    "    if done >= total:\n",
    "        break\n",
    "    time.sleep(2)\n",
    "print(\"‚úÖ HNSW graph fully built!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "611f2e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=None indexed_vectors_count=0 points_count=500 segments_count=8 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=384, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None, strict_mode_config=StrictModeConfigOutput(enabled=False, max_query_limit=None, max_timeout=None, unindexed_filtering_retrieve=None, unindexed_filtering_update=None, search_max_hnsw_ef=None, search_allow_exact=None, search_max_oversampling=None, upsert_max_batchsize=None, max_collection_vector_size_bytes=None, read_rate_limit=None, write_rate_limit=None, max_collection_payload_size_bytes=None, max_points_count=None, filter_max_conditions=None, condition_max_size=None, multivector_config=None, sparse_config=None)) payload_schema={'employee_growth_yoy': PayloadIndexInfo(data_type=<PayloadSchemaType.FLOAT: 'float'>, params=None, points=0), 'competitors': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'revenue_estimate_annual': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'latest_funding_round_type': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'industry_sector': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'board_members_advisors': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=0), 'primary_products_services': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=0), 'press_mentions_recent_news': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=0), 'state': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'number_of_employees_current': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'year_founded': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'number_of_employees_estimate_range': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'company_name': PayloadIndexInfo(data_type=<PayloadSchemaType.TEXT: 'text'>, params=None, points=500), 'logo_url': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'founders': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'headquarters_city': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'legal_entity_type': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'hiring_status': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'target_market': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'company_website': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'total_funding_raised_inr': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'number_of_funding_rounds': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'product_categories': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'valuation_estimate_if_available': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'tech_stack': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'integrations_apis_offered': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=0), 'latest_funding_date': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'popular_roles_open': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'major_customers_logos': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=0), 'company_description_long': PayloadIndexInfo(data_type=<PayloadSchemaType.TEXT: 'text'>, params=None, points=500), 'company_description_short': PayloadIndexInfo(data_type=<PayloadSchemaType.TEXT: 'text'>, params=None, points=500), 'key_people': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'lead_investors': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500)}\n",
      "{'employee_growth_yoy': PayloadIndexInfo(data_type=<PayloadSchemaType.FLOAT: 'float'>, params=None, points=0), 'competitors': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'revenue_estimate_annual': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'latest_funding_round_type': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'industry_sector': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'board_members_advisors': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=0), 'primary_products_services': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=0), 'press_mentions_recent_news': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=0), 'state': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'number_of_employees_current': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'year_founded': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'number_of_employees_estimate_range': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'company_name': PayloadIndexInfo(data_type=<PayloadSchemaType.TEXT: 'text'>, params=None, points=500), 'logo_url': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'founders': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'headquarters_city': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'legal_entity_type': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'hiring_status': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'target_market': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'company_website': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'total_funding_raised_inr': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'number_of_funding_rounds': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'product_categories': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'valuation_estimate_if_available': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'tech_stack': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'integrations_apis_offered': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=0), 'latest_funding_date': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'popular_roles_open': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'major_customers_logos': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=0), 'company_description_long': PayloadIndexInfo(data_type=<PayloadSchemaType.TEXT: 'text'>, params=None, points=500), 'company_description_short': PayloadIndexInfo(data_type=<PayloadSchemaType.TEXT: 'text'>, params=None, points=500), 'key_people': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'lead_investors': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500)}\n"
     ]
    }
   ],
   "source": [
    "## Inspecting the Collection & Payload Schema\n",
    "\n",
    "# 1Ô∏è‚É£ See the vector-config & overall collection info\n",
    "collection_info = client.get_collection(collection_name=COLLECTION_NAME)\n",
    "print(collection_info)\n",
    "\n",
    "# 2Ô∏è‚É£ Peek at the payload schema you just registered\n",
    "print(collection_info.payload_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccc7bd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact‚Äêmatch ‚Äòstate=Maharashtra‚Äô ‚Üí 0 hit(s):\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client.http.models import Filter, FieldCondition, MatchValue\n",
    "\n",
    "qf = Filter(must=[\n",
    "    FieldCondition(key=\"state\", match=MatchValue(value=\"Maharashtra\"))\n",
    "])\n",
    "\n",
    "hits = client.scroll(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    scroll_filter=qf,\n",
    "    with_payload=True,\n",
    "    limit=5\n",
    ")[0]\n",
    "\n",
    "print(f\"Exact‚Äêmatch ‚Äòstate=Maharashtra‚Äô ‚Üí {len(hits)} hit(s):\")\n",
    "for pt in hits:\n",
    "    print(\" \", pt.payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ec500d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter year_founded in [2000,2005] ‚Üí 0 hits\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client.http.models import Filter, FieldCondition, Range\n",
    "\n",
    "def test_range_filter(field: str, gte=None, lte=None, limit=10):\n",
    "    r = Range(gte=gte, lte=lte)\n",
    "    qf = Filter(must=[ FieldCondition(key=field, range=r) ])\n",
    "    hits = client.scroll(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        scroll_filter=qf,\n",
    "        with_payload=True,\n",
    "        limit=limit,\n",
    "    )[0]\n",
    "    print(f\"Filter {field} in [{gte},{lte}] ‚Üí {len(hits)} hits\")\n",
    "    for pt in hits:\n",
    "        print(\" \", pt.payload)\n",
    "\n",
    "# e.g. companies founded between 2000 and 2005\n",
    "test_range_filter(\"year_founded\", gte=2000, lte=2005)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7756937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SRC Path: /home/prashant-agrawal/projects/netflix_talk2data/src\n"
     ]
    }
   ],
   "source": [
    "# %% ðŸ“ Souring Path\n",
    "import sys, os\n",
    "SRC_PATH = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"src\"))\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH)\n",
    "print(f\"âœ… SRC Path: {SRC_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56eb004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qdrant store path: /home/prashant-agrawal/projects/netflix_talk2data/src/database/qdrant_store_local_db/collection\n",
      "Data path: /home/prashant-agrawal/projects/netflix_talk2data/src/Data/Enriched_Indian_Startup_Dataset.csv\n",
      "Schema path: /home/prashant-agrawal/projects/netflix_talk2data/src/schema/payload_schema.json\n",
      "ðŸ“Œ Base Dir: /home/prashant-agrawal/projects/netflix_talk2data/src\n",
      "ðŸ“Œ CSV Path: /home/prashant-agrawal/projects/netflix_talk2data/src/Data/Enriched_Indian_Startup_Dataset.csv\n",
      "ðŸ“Œ Qdrant Local Path: /home/prashant-agrawal/projects/netflix_talk2data/src/database/qdrant_store_local_db/collection\n",
      "ðŸ“Œ Collection Name: indian_startups\n",
      "ðŸ“Œ Schema Path: /home/prashant-agrawal/projects/netflix_talk2data/src/schema/payload_schema.json\n"
     ]
    }
   ],
   "source": [
    "# Python imports & libraries\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from qdrant_client.http.models import Range\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance, PayloadSchemaType, FieldCondition, MatchValue, Filter\n",
    "from typing import Dict, Union, Any\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "\n",
    "# ðŸš€ Import your utility loaders\n",
    "from utils.qdrant_client_loader import get_qdrant_collection_name\n",
    "from utils.path_config import get_base_dir, get_data_path, get_qdrant_store_path, get_schema_path\n",
    "\n",
    "# %% ðŸ“ Paths\n",
    "BASE_DIR = get_base_dir()\n",
    "DATA_PATH = get_data_path()\n",
    "SCHEMA_OUTPUT_PATH = get_schema_path()\n",
    "qdrant_store_path = get_qdrant_store_path()\n",
    "COLLECTION_NAME = get_qdrant_collection_name()\n",
    "\n",
    "print(f\"ðŸ“Œ Base Dir: {BASE_DIR}\")\n",
    "print(f\"ðŸ“Œ CSV Path: {DATA_PATH}\")\n",
    "print(f\"ðŸ“Œ Qdrant Local Path: {qdrant_store_path}\")\n",
    "print(f\"ðŸ“Œ Collection Name: {COLLECTION_NAME}\")\n",
    "print(f\"ðŸ“Œ Schema Path: {SCHEMA_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b5a09e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utility: Normalization ---\n",
    "def normalize_field_name(field: str) -> str:\n",
    "    return (\n",
    "        field.strip().lower()\n",
    "        .replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        .replace(\"/\", \"_\")\n",
    "    )\n",
    "\n",
    "def normalize_field_value(value) -> str:\n",
    "    return str(value).strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2378bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load & Process Data ---\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df = df.dropna(subset=[\"Company Description (Long)\"]).reset_index(drop=True)\n",
    "\n",
    "def build_points(df):\n",
    "    for idx, row in df.iterrows():\n",
    "        metadata = {\n",
    "            normalize_field_name(str(k)): normalize_field_value(v)\n",
    "            for k, v in row.items() if pd.notna(v)\n",
    "        }\n",
    "        # Use only main description as page_content\n",
    "        content = str(row[\"Company Description (Long)\"]) if \"Company Description (Long)\" in row else \"\"\n",
    "        yield {\n",
    "            \"id\": int(idx),\n",
    "            \"vector\": embedding_model.embed_query(content),\n",
    "            \"payload\": metadata\n",
    "        }\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4335c26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 2. Qdrant Setup ---\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "if client.collection_exists(COLLECTION_NAME):\n",
    "    client.delete_collection(collection_name=COLLECTION_NAME)\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21e15a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SRC Path: /home/prashant-agrawal/projects/netflix_talk2data/src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from qdrant_client.http.models import PayloadSchemaType\n",
    "from schema.qdrant_schema import PAYLOAD_SCHEMA\n",
    "\n",
    "for field, schema in PAYLOAD_SCHEMA.items():\n",
    "    # Normalize names (snake_case, all lowercase)\n",
    "    if isinstance(schema, dict) and \"type\" in schema:\n",
    "        field_schema = schema[\"type\"]\n",
    "    else:\n",
    "        field_schema = schema\n",
    "    client.create_payload_index(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        field_name=field,\n",
    "        field_schema=field_schema\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4f1db35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ingested 500 points into indian_startups.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Upload Data ---\n",
    "points = list(build_points(df))\n",
    "client.upsert(collection_name=COLLECTION_NAME, points=points)\n",
    "print(f\"âœ… Ingested {len(points)} points into {COLLECTION_NAME}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "611f2e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=None indexed_vectors_count=0 points_count=1000 segments_count=8 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=1536, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None, strict_mode_config=StrictModeConfigOutput(enabled=False, max_query_limit=None, max_timeout=None, unindexed_filtering_retrieve=None, unindexed_filtering_update=None, search_max_hnsw_ef=None, search_allow_exact=None, search_max_oversampling=None, upsert_max_batchsize=None, max_collection_vector_size_bytes=None, read_rate_limit=None, write_rate_limit=None, max_collection_payload_size_bytes=None, max_points_count=None, filter_max_conditions=None, condition_max_size=None, multivector_config=None, sparse_config=None)) payload_schema={'year_founded': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'industry_sector': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'number_of_funding_rounds': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'latest_funding_round_type': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'board_members_advisors': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=0), 'founders': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'number_of_employees_estimate_range': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'legal_entity_type': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'total_funding_raised_inr': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'logo_url': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'popular_roles_open': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'hiring_status': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'revenue_estimate_annual': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'product_categories': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'competitors': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'number_of_employees_current': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'target_market': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'company_website': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'key_people': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'company_name': PayloadIndexInfo(data_type=<PayloadSchemaType.TEXT: 'text'>, params=None, points=500), 'valuation_estimate_if_available': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'state': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'company_description_short': PayloadIndexInfo(data_type=<PayloadSchemaType.TEXT: 'text'>, params=None, points=500), 'press_mentions_recent_news': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=0), 'company_description_long': PayloadIndexInfo(data_type=<PayloadSchemaType.TEXT: 'text'>, params=None, points=500), 'primary_products_services': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=0), 'integrations_apis_offered': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=0), 'headquarters_city': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'employee_growth_yoy': PayloadIndexInfo(data_type=<PayloadSchemaType.FLOAT: 'float'>, params=None, points=0), 'major_customers_logos': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=0), 'lead_investors': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'tech_stack': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'latest_funding_date': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500)}\n",
      "{'year_founded': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'industry_sector': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'number_of_funding_rounds': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'latest_funding_round_type': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'board_members_advisors': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=0), 'founders': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'number_of_employees_estimate_range': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'legal_entity_type': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'total_funding_raised_inr': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'logo_url': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'popular_roles_open': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'hiring_status': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'revenue_estimate_annual': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'product_categories': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'competitors': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'number_of_employees_current': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'target_market': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'company_website': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'key_people': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'company_name': PayloadIndexInfo(data_type=<PayloadSchemaType.TEXT: 'text'>, params=None, points=500), 'valuation_estimate_if_available': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=0), 'state': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'company_description_short': PayloadIndexInfo(data_type=<PayloadSchemaType.TEXT: 'text'>, params=None, points=500), 'press_mentions_recent_news': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=0), 'company_description_long': PayloadIndexInfo(data_type=<PayloadSchemaType.TEXT: 'text'>, params=None, points=500), 'primary_products_services': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=0), 'integrations_apis_offered': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=0), 'headquarters_city': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'employee_growth_yoy': PayloadIndexInfo(data_type=<PayloadSchemaType.FLOAT: 'float'>, params=None, points=0), 'major_customers_logos': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=0), 'lead_investors': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'tech_stack': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500), 'latest_funding_date': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=500)}\n"
     ]
    }
   ],
   "source": [
    "## Inspecting the Collection & Payload Schema\n",
    "\n",
    "# 1ï¸âƒ£ See the vector-config & overall collection info\n",
    "collection_info = client.get_collection(collection_name=COLLECTION_NAME)\n",
    "print(collection_info)\n",
    "\n",
    "# 2ï¸âƒ£ Peek at the payload schema you just registered\n",
    "print(collection_info.payload_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccc7bd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactâ€match â€˜state=delhiâ€™ â†’ 5 hit(s):\n",
      "  {'company_name': 'noise', 'legal_entity_type': 'proprietorship', 'state': 'delhi', 'headquarters_city': 'mumbai', 'year_founded': '1996', 'company_website': 'https://perkinsllc.in', 'logo_url': 'https://logo.clearbit.com/perkinsllc.in', 'company_description_short': 'streamlined needs-based flexibility', 'company_description_long': 'again customer performance director sure media. boy seat however road area shake if.\\r\\nmouth chance believe fill sometimes those necessary various. serve quality happy under.\\r\\nhistory full energy our allow. under marriage last represent night.\\r\\nstate vote heavy art hope political five.', 'industry_sector': 'e-commerce', 'total_funding_raised_inr': 'â‚¹115 cr', 'number_of_funding_rounds': '2', 'latest_funding_round_type': 'series b', 'latest_funding_date': '2020-07-18', 'lead_investors': 'smith, zhang and walker', 'revenue_estimate_annual': 'â‚¹108 cr', 'valuation_estimate_if_available': 'â‚¹679 cr', 'number_of_employees_current': '472', 'number_of_employees_estimate_range': '225-1672', 'key_people': 'ceo: william watts, cto: kristin williams', 'founders': 'sandra rush (https://linkedin.com/in/qmartin)', 'board_members___advisors': 'brian thomas', 'employee_growth_yoy_%': '42%', 'hiring_status': 'actively hiring', 'popular_roles_open': 'engineering, product', 'primary_products___services': 'credit card rewards', 'product_categories': 'b2b', 'tech_stack': 'python, react, aws', 'integrations___apis_offered': 'razorpay, zoho, tally', 'target_market': 'consumers', 'major_customers___logos': 'direct consumers', 'press_mentions___recent_news': 'ten bed positive property order central.', 'competitors': 'boat, fire-boltt'}\n",
      "  {'company_name': 'practically', 'legal_entity_type': 'llc', 'state': 'delhi', 'headquarters_city': 'ahmedabad', 'year_founded': '2010', 'company_website': 'https://bender,adamsandaguilar.in', 'logo_url': 'https://logo.clearbit.com/bender,adamsandaguilar.in', 'company_description_short': 'persistent directional policy', 'company_description_long': 'trial official court involve to around cost. year push help compare medical group structure. like actually agreement option. inside dinner big drop believe thousand pattern.', 'industry_sector': 'e-commerce', 'total_funding_raised_inr': 'â‚¹447 cr', 'number_of_funding_rounds': '4', 'latest_funding_round_type': 'series b', 'latest_funding_date': '2023-06-30', 'lead_investors': 'patton, clark and cruz', 'revenue_estimate_annual': 'â‚¹131 cr', 'valuation_estimate_if_available': 'â‚¹1126 cr', 'number_of_employees_current': '55', 'number_of_employees_estimate_range': '43-1219', 'key_people': 'ceo: brett martin, cto: stephanie mitchell', 'founders': 'adam brown (https://linkedin.com/in/thomasrivera)', 'board_members___advisors': 'jesus mahoney', 'employee_growth_yoy_%': '19%', 'hiring_status': 'actively hiring', 'popular_roles_open': 'product, sales', 'primary_products___services': 'home services', 'product_categories': 'd2c', 'tech_stack': 'python, react, aws', 'integrations___apis_offered': 'razorpay, zoho, tally', 'target_market': 'consumers', 'major_customers___logos': 'direct consumers', 'press_mentions___recent_news': 'once democrat member about both to.', 'competitors': 'company x, company y'}\n",
      "  {'company_name': 'tork motors', 'legal_entity_type': 'proprietorship', 'state': 'delhi', 'headquarters_city': 'hyderabad', 'year_founded': '2004', 'company_website': 'https://lutz-maddox.in', 'logo_url': 'https://logo.clearbit.com/lutz-maddox.in', 'company_description_short': 'exclusive user-facing help-desk', 'company_description_long': 'protect site pass bring training still. late hair stay phone mention. certainly many much rest walk. natural table that guy here alone young.\\r\\nwear not agency inside large.\\r\\neconomic us area. measure hold determine first develop prevent box. them blue strong. voice begin future land.', 'industry_sector': 'agritech', 'total_funding_raised_inr': 'â‚¹113 cr', 'number_of_funding_rounds': '1', 'latest_funding_round_type': 'seed', 'latest_funding_date': '2023-07-29', 'lead_investors': 'terrell-castillo', 'revenue_estimate_annual': 'â‚¹10 cr', 'valuation_estimate_if_available': 'â‚¹1355 cr', 'number_of_employees_current': '55', 'number_of_employees_estimate_range': '131-1874', 'key_people': 'ceo: debra beck, cto: tristan gonzalez', 'founders': 'melissa cross (https://linkedin.com/in/bradleykeller)', 'board_members___advisors': 'david diaz', 'employee_growth_yoy_%': '18%', 'hiring_status': 'actively hiring', 'popular_roles_open': 'product, engineering', 'primary_products___services': 'online grocery', 'product_categories': 'b2b', 'tech_stack': 'python, react, aws', 'integrations___apis_offered': 'razorpay, zoho, tally', 'target_market': 'consumers', 'major_customers___logos': 'direct consumers', 'press_mentions___recent_news': 'nor culture near history ever unit.', 'competitors': 'company x, company y'}\n",
      "  {'company_name': 'mamaearth', 'legal_entity_type': 'public ltd', 'state': 'delhi', 'headquarters_city': 'delhi', 'year_founded': '2012', 'company_website': 'https://thompsongroup.in', 'logo_url': 'https://logo.clearbit.com/thompsongroup.in', 'company_description_short': 'front-line logistical service-desk', 'company_description_long': 'people that someone laugh. particularly onto forward movement despite.\\r\\nmachine reach generation item time memory agent. world dream behind president serve person more. guess responsibility agent film point none.', 'industry_sector': 'e-commerce', 'total_funding_raised_inr': 'â‚¹35 cr', 'number_of_funding_rounds': '3', 'latest_funding_round_type': 'series c', 'latest_funding_date': '2021-04-11', 'lead_investors': 'michael, mcneil and james', 'revenue_estimate_annual': 'â‚¹81 cr', 'valuation_estimate_if_available': 'â‚¹439 cr', 'number_of_employees_current': '416', 'number_of_employees_estimate_range': '272-600', 'key_people': 'ceo: charles johnson, cto: christopher contreras', 'founders': 'patricia hunt (https://linkedin.com/in/mwilson)', 'board_members___advisors': 'linda nichols', 'employee_growth_yoy_%': '16%', 'hiring_status': 'actively hiring', 'popular_roles_open': 'sales, marketing', 'primary_products___services': 'electric scooters', 'product_categories': 'd2c', 'tech_stack': 'node.js, mongodb, azure', 'integrations___apis_offered': 'razorpay, zoho, tally', 'target_market': 'consumers', 'major_customers___logos': 'direct consumers', 'press_mentions___recent_news': 'campaign read yet quickly myself discover country.', 'competitors': 'wow skin science, plum'}\n",
      "  {'company_name': 'scripbox', 'legal_entity_type': 'llc', 'state': 'delhi', 'headquarters_city': 'chandigarh', 'year_founded': '2013', 'company_website': 'https://peters-allen.in', 'logo_url': 'https://logo.clearbit.com/peters-allen.in', 'company_description_short': 'monitored bandwidth-monitored extranet', 'company_description_long': 'catch tv sell condition. identify adult statement board power feel site. whether get factor idea tax build grow.\\r\\nwater meet happy. feel conference near doctor recently by often. painting subject company line.\\r\\ndog for reality. piece charge only air gas.', 'industry_sector': 'fintech', 'total_funding_raised_inr': 'â‚¹16 cr', 'number_of_funding_rounds': '4', 'latest_funding_round_type': 'series a', 'latest_funding_date': '2024-09-10', 'lead_investors': 'williams, dominguez and todd', 'revenue_estimate_annual': 'â‚¹62 cr', 'valuation_estimate_if_available': 'â‚¹1101 cr', 'number_of_employees_current': '493', 'number_of_employees_estimate_range': '227-1648', 'key_people': 'ceo: jeffrey barnes, cto: vanessa cole', 'founders': 'andrew gray (https://linkedin.com/in/sloanmichelle)', 'board_members___advisors': 'lindsey rojas', 'employee_growth_yoy_%': '9%', 'hiring_status': 'hiring freeze', 'popular_roles_open': 'engineering, marketing', 'primary_products___services': 'insurance platform', 'product_categories': 'd2c', 'tech_stack': 'java, spring boot, gcp', 'integrations___apis_offered': 'razorpay, zoho, tally', 'target_market': 'enterprises', 'major_customers___logos': 'unilever, mahindra', 'press_mentions___recent_news': 'bit school doctor never.', 'competitors': 'startup a, startup b'}\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client.http.models import Filter, FieldCondition, MatchValue\n",
    "\n",
    "qf = Filter(must=[\n",
    "    FieldCondition(key=\"state\", match=MatchValue(value=\"delhi\"))\n",
    "])\n",
    "\n",
    "hits = client.scroll(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    scroll_filter=qf,\n",
    "    with_payload=True,\n",
    "    limit=5\n",
    ")[0]\n",
    "\n",
    "print(f\"Exactâ€match â€˜state=delhiâ€™ â†’ {len(hits)} hit(s):\")\n",
    "for pt in hits:\n",
    "    print(\" \", pt.payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ec500d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter year_founded in [2000,2005] â†’ 0 hits\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client.http.models import Filter, FieldCondition, Range\n",
    "\n",
    "def test_range_filter(field: str, gte=None, lte=None, limit=10):\n",
    "    r = Range(gte=gte, lte=lte)\n",
    "    qf = Filter(must=[ FieldCondition(key=field, range=r) ])\n",
    "    hits = client.scroll(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        scroll_filter=qf,\n",
    "        with_payload=True,\n",
    "        limit=limit,\n",
    "    )[0]\n",
    "    print(f\"Filter {field} in [{gte},{lte}] â†’ {len(hits)} hits\")\n",
    "    for pt in hits:\n",
    "        print(\" \", pt.payload)\n",
    "\n",
    "# e.g. companies founded between 2000 and 2005\n",
    "test_range_filter(\"year_founded\", gte=2000, lte=2005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f018de90",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Document\npage_content\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, doc\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# e.g.\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mhybrid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfintech startups\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelhi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 29\u001b[0m, in \u001b[0;36mhybrid_search\u001b[0;34m(query, filters, k)\u001b[0m\n\u001b[1;32m     24\u001b[0m conds \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     25\u001b[0m     FieldCondition(key\u001b[38;5;241m=\u001b[39mf, match\u001b[38;5;241m=\u001b[39mMatchValue(value\u001b[38;5;241m=\u001b[39mv))\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f,v \u001b[38;5;129;01min\u001b[39;00m filters\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     27\u001b[0m ]\n\u001b[1;32m     28\u001b[0m qf \u001b[38;5;241m=\u001b[39m Filter(must\u001b[38;5;241m=\u001b[39mconds)\n\u001b[0;32m---> 29\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mqdrant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m, filters=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m â†’ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m hits\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc,score \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m~/projects/netflix_talk2data/venv/lib/python3.10/site-packages/langchain_community/vectorstores/qdrant.py:364\u001b[0m, in \u001b[0;36mQdrant.similarity_search_with_score\u001b[0;34m(self, query, k, filter, search_params, offset, score_threshold, consistency, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msimilarity_search_with_score\u001b[39m(\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    321\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m    330\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m        List of documents most similar to the query text and distance for each.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score_by_vector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[43msearch_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconsistency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsistency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/netflix_talk2data/venv/lib/python3.10/site-packages/langchain_community/vectorstores/qdrant.py:623\u001b[0m, in \u001b[0;36mQdrant.similarity_search_with_score_by_vector\u001b[0;34m(self, embedding, k, filter, search_params, offset, score_threshold, consistency, **kwargs)\u001b[0m\n\u001b[1;32m    608\u001b[0m     query_vector \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_name, embedding)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    610\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39msearch(\n\u001b[1;32m    611\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollection_name,\n\u001b[1;32m    612\u001b[0m     query_vector\u001b[38;5;241m=\u001b[39mquery_vector,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    622\u001b[0m )\n\u001b[0;32m--> 623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    624\u001b[0m     (\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_document_from_scored_point(\n\u001b[1;32m    626\u001b[0m             result,\n\u001b[1;32m    627\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollection_name,\n\u001b[1;32m    628\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent_payload_key,\n\u001b[1;32m    629\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata_payload_key,\n\u001b[1;32m    630\u001b[0m         ),\n\u001b[1;32m    631\u001b[0m         result\u001b[38;5;241m.\u001b[39mscore,\n\u001b[1;32m    632\u001b[0m     )\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    634\u001b[0m ]\n",
      "File \u001b[0;32m~/projects/netflix_talk2data/venv/lib/python3.10/site-packages/langchain_community/vectorstores/qdrant.py:625\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    608\u001b[0m     query_vector \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_name, embedding)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    610\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39msearch(\n\u001b[1;32m    611\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollection_name,\n\u001b[1;32m    612\u001b[0m     query_vector\u001b[38;5;241m=\u001b[39mquery_vector,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    622\u001b[0m )\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    624\u001b[0m     (\n\u001b[0;32m--> 625\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_document_from_scored_point\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent_payload_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata_payload_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    631\u001b[0m         result\u001b[38;5;241m.\u001b[39mscore,\n\u001b[1;32m    632\u001b[0m     )\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    634\u001b[0m ]\n",
      "File \u001b[0;32m~/projects/netflix_talk2data/venv/lib/python3.10/site-packages/langchain_community/vectorstores/qdrant.py:2004\u001b[0m, in \u001b[0;36mQdrant._document_from_scored_point\u001b[0;34m(cls, scored_point, collection_name, content_payload_key, metadata_payload_key)\u001b[0m\n\u001b[1;32m   2002\u001b[0m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m scored_point\u001b[38;5;241m.\u001b[39mid\n\u001b[1;32m   2003\u001b[0m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_collection_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m collection_name\n\u001b[0;32m-> 2004\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDocument\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpage_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscored_point\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_payload_key\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2007\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/netflix_talk2data/venv/lib/python3.10/site-packages/langchain_core/documents/base.py:278\u001b[0m, in \u001b[0;36mDocument.__init__\u001b[0;34m(self, page_content, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Pass page_content in as positional or named arg.\"\"\"\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# my-py is complaining that page_content is not defined on the base class.\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# Here, we're relying on pydantic base class to handle the validation.\u001b[39;00m\n\u001b[0;32m--> 278\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpage_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/netflix_talk2data/venv/lib/python3.10/site-packages/langchain_core/load/serializable.py:130\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/netflix_talk2data/venv/lib/python3.10/site-packages/pydantic/main.py:253\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    252\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    255\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    259\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    260\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Document\npage_content\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type"
     ]
    }
   ],
   "source": [
    "# if you also want to test your similarity_search_with_score call:\n",
    "from qdrant_client.http.models import FieldCondition, MatchValue\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Convert DataFrame rows to langchain Document objects, ensuring page_content is a string\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=str(row[\"Company Description (Long)\"]) if pd.notna(row[\"Company Description (Long)\"]) else \"\",\n",
    "        metadata={col: row[col] for col in df.columns if col != \"Company Description (Long)\"}\n",
    "    )\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "qdrant = Qdrant.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    url=\"http://localhost:6333\",\n",
    "    collection_name=COLLECTION_NAME,\n",
    ")\n",
    "\n",
    "def hybrid_search(query: str, filters: dict, k: int = 5):\n",
    "    from qdrant_client.http.models import Filter, FieldCondition, MatchValue\n",
    "    conds = [\n",
    "        FieldCondition(key=f, match=MatchValue(value=v))\n",
    "        for f,v in filters.items()\n",
    "    ]\n",
    "    qf = Filter(must=conds)\n",
    "    results = qdrant.similarity_search_with_score(query=query, k=k, filter=qf)\n",
    "    print(f\"Query={query!r}, filters={filters} â†’ {len(results)} hits\")\n",
    "    for doc,score in results:\n",
    "        print(f\"  {score:.3f}\", doc.metadata)\n",
    "\n",
    "# e.g.\n",
    "hybrid_search(\"fintech startups\", {\"state\":\"delhi\"}, k=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

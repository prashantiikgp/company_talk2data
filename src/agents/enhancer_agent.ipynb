{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94714258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-core in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (0.3.61)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: langchain-openai in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (0.3.18)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from langchain) (2.11.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from langchain) (0.3.42)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from langchain-core) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from langchain-core) (4.13.2)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0\n",
      "  Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Collecting numpy>=1.26.2\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting httpx-sse<1.0.0,>=0.4.0\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from langchain-openai) (1.82.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from langchain-openai) (0.9.0)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m333.9/333.9 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.3/287.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: tqdm>4 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Collecting python-dotenv>=0.21.0\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: greenlet>=1 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/prashant-agrawal/projects/netflix_talk2data/venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: python-dotenv, propcache, numpy, mypy-extensions, multidict, marshmallow, httpx-sse, frozenlist, aiohappyeyeballs, yarl, typing-inspect, aiosignal, pydantic-settings, dataclasses-json, aiohttp, langchain-community\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.0 aiosignal-1.3.2 dataclasses-json-0.6.7 frozenlist-1.6.0 httpx-sse-0.4.0 langchain-community-0.3.24 marshmallow-3.26.1 multidict-6.4.4 mypy-extensions-1.1.0 numpy-2.2.6 propcache-0.3.1 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0 yarl-1.20.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain langchain-core langchain-community langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "594a8f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC_DIR: /home/prashant-agrawal/projects/netflix_talk2data/src\n"
     ]
    }
   ],
   "source": [
    "# Ensure src/ is in sys.path so 'tools' can be imported\n",
    "import os\n",
    "import sys\n",
    "SRC_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if SRC_DIR not in sys.path:\n",
    "    sys.path.insert(0, SRC_DIR)\n",
    "\n",
    "print(f\"SRC_DIR: {SRC_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba47942e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qdrant store path: /home/prashant-agrawal/projects/netflix_talk2data/src/database/qdrant_store_local_db/collection\n",
      "Data path: /home/prashant-agrawal/projects/netflix_talk2data/src/Data/Enriched_Indian_Startup_Dataset.csv\n",
      "Schema path: /home/prashant-agrawal/projects/netflix_talk2data/src/schema/payload_schema.json\n",
      "Vector store path: /home/prashant-agrawal/projects/netflix_talk2data/src/database/vector_store/faiss_full_row_index\n",
      "Base directory: /home/prashant-agrawal/projects/netflix_talk2data/src\n",
      "FAISS index loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# -- Enhancer Agent --\n",
    "# This script sets up an agent that enhances queries by extracting structured metadata and filters.\n",
    "# It uses a set of tools to analyze and transform vague or unstructured queries into clear, structured metadata.\n",
    "# The agent is designed to work with the LangChain framework and utilizes OpenAI's GPT-4 model.\n",
    "# The agent is capable of using various tools such as keyword extractors, numeric constraint extractors, and category classifiers.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from langchain_core.tools import Tool\n",
    "from langchain.agents import create_react_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.types import Command\n",
    "from typing import Literal\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# ğŸ” Import all tools from registry\n",
    "from tools.enhancer_tools_registry import (\n",
    "    keyword_extractor_tool,\n",
    "    extract_numeric_constraints_tool,\n",
    "    filter_composer_tool\n",
    ")\n",
    "\n",
    "# Define tools for the enhancer agent\n",
    "enhancer_tools = [\n",
    "    keyword_extractor_tool,\n",
    "    extract_numeric_constraints_tool,\n",
    "    filter_composer_tool\n",
    "]\n",
    "\n",
    "\n",
    "# Define tool names for the agent\n",
    "tool_names = [tool.name for tool in enhancer_tools]\n",
    "\n",
    "# Define the tool descriptions\n",
    "tool_descriptions = [tool.description for tool in enhancer_tools]\n",
    "\n",
    "# Build readable tool help text for the prompt\n",
    "tool_help_text = \"\\n\".join(\n",
    "    [f\"{i+1}. {tool.name} - {tool.description}\" for i, tool in enumerate(enhancer_tools)]\n",
    ")\n",
    "\n",
    "\n",
    "# Define system prompt used during agent creation\n",
    "\n",
    "enhancer_agent_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"You are a Query Enhancer Agent.\n",
    "\n",
    "Your task is to:\n",
    "1. Understand the user's query and its intent.\n",
    "2. Clarify any ambiguities or vague instructions.\n",
    "3. Add contextual details such as location, industry, amount, category, or other metadata if implied.\n",
    "4. Output an improved version of the query, optimized for semantic and structured search.\n",
    "\n",
    "Rules:\n",
    "- Do not ask follow-up questions.\n",
    "- Infer likely meanings when user input is ambiguous.\n",
    "- Your output should be a standalone, precise query in natural language.\n",
    "- Do not include explanations or formatting like â€œHereâ€™s the improved query.â€\n",
    "\n",
    "Example:\n",
    "1. Fuzzy Funding Reference â†’ Structured Numeric + Stage Filter\n",
    "User Query: \"Startups with big funding\"\n",
    "Enhanced Prompt: \"Indian startups that raised over $10 million in Series A or later funding rounds.\"\n",
    "Focus: Adds monetary threshold and filters funding stage.\n",
    "\n",
    "2. Location Mention â†’ Structured Geographic Filter\n",
    "User Query: \"Top SaaS companies in Bangalore\"\n",
    "Enhanced Prompt: \"Top-performing SaaS startups headquartered in Bengaluru with recent traction or funding activity.\"\n",
    "Focus: Resolves city + domain, adds optional temporal or traction filter.\n",
    "\n",
    "ğŸ”¹ 3. Team Size Mention â†’ Employee Count Filter\n",
    "User Query: \"Companies with large teams working in AI\"\n",
    "Enhanced Prompt: \"AI-focused startups in India with over 200 employees and active hiring status.\"\n",
    "Focus: Converts â€œlarge teamâ€ into a numeric filter + hiring signal.\n",
    "\n",
    "ğŸ”¹ 4. Recent Growth Mention â†’ Year + Funding Filter\n",
    "User Query: \"Fast-growing e-commerce startups\"\n",
    "Enhanced Prompt: \"E-commerce startups founded after 2018 with rapid growth and multiple funding rounds.\"\n",
    "Focus: Infers growth using founding year and number of rounds.\n",
    "\n",
    "ğŸ”¹ 5. Tech Stack Mention â†’ Tool + Sector Filters\n",
    "User Query: \"Fintech companies using AWS and React\"\n",
    "Enhanced Prompt: \"Fintech startups with a tech stack including AWS and React, offering scalable B2C solutions.\"\n",
    "Focus: Extracts cloud/backend/frontend tech + sector.\n",
    "\n",
    "\n",
    "\n",
    "You have access to the following tools:\n",
    "{tools}\n",
    "\n",
    "Format:\n",
    "Question: the input query\n",
    "Thought: think step-by-step about what to extract\n",
    "Action: the tool to use, from [{tool_names}]\n",
    "Action Input: JSON string or plain text input to the tool\n",
    "Observation: result returned by the tool\n",
    "... (repeat Thought/Action/Observation as needed)\n",
    "Thought: I have gathered all necessary structured data.\n",
    "Final Answer: a dictionary of all extracted metadata and filters\n",
    "\n",
    "Constraints:\n",
    "- NEVER ask the user again\n",
    "- ONLY use tools\n",
    "- NEVER hallucinate missing data\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    ")\n",
    "\n",
    "# Format the prompt with tool descriptions and names\n",
    "formatted_prompt = enhancer_agent_prompt_template.partial(\n",
    "    tools=tool_help_text,\n",
    "    tool_names=\", \".join(tool.name for tool in enhancer_tools)\n",
    ")\n",
    "\n",
    "# ğŸ”§ Define the React-style agent\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")  # Or use your preferred model\n",
    "\n",
    "\n",
    "# Create the agent\n",
    "enhancer_agent = create_react_agent(\n",
    "    llm=llm,\n",
    "    tools=enhancer_tools,\n",
    "    prompt=formatted_prompt,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adc1c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhancer_node(state: dict) -> Command[Literal[\"supervisor\"]]:\n",
    "    \"\"\"\n",
    "    Enhancer node for refining and structuring vague user queries.\n",
    "\n",
    "    Returns:\n",
    "        Command: Enhanced message sent back to the supervisor with reasoning.\n",
    "    \"\"\"\n",
    "\n",
    "    # ğŸ§¾ Instructional system message â€” passed as part of prompt to guide behavior\n",
    "    system_prompt = (\n",
    "        \"You are an advanced query enhancer. Your task is to:\\n\"\n",
    "        \"- Clarify and refine vague or ambiguous user inputs.\\n\"\n",
    "        \"- Extract metadata using tools (industry, funding stage, filters).\\n\"\n",
    "        \"- NEVER generate final answers, only enhance the query.\\n\"\n",
    "        \"- Return your reasoning and structured results back to the supervisor.\"\n",
    "    )\n",
    "\n",
    "    # ğŸ§  Compose messages for the agent\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "\n",
    "    # ğŸš€ Invoke the agent (you can use `invoke()` or stream if needed)\n",
    "    enhanced_response = enhancer_agent.invoke(messages)\n",
    "\n",
    "    print(\"ğŸ§© Enhancer Node executed. Routing back to: supervisor\")\n",
    "\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(\n",
    "                    content=enhanced_response.content,\n",
    "                    name=\"enhancer\"\n",
    "                )\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96e729ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The query involves several structured elements: company type (D2C or SaaS), locations (Delhi or Hyderabad), funding amount (over â‚¹200 crore), hiring status (hiring for engineers and PMs), valuation (above $500 million), product offerings (mobile apps or APIs), and investors (backed by Sequoia or Accel). I will start by extracting the keywords related to these fields.\n",
      "\n",
      "Action: keyword_extractor\n",
      "Action Input: \"List D2C or SaaS companies in Delhi or Hyderabad that raised over â‚¹200 crore, are currently hiring for engineers and PMs, valued above $500 million, offer mobile apps or APIs, and are backed by Sequoia or Accel.\"\u001b[0m\u001b[36;1m\u001b[1;3m{'headquarters_city': ['delhi', 'hyderabad'], 'industry_sector': ['saas', 'd2c'], 'total_funding_raised_inr': ['â‚¹', 'raised', 'crore', 'cr', 'over â‚¹', 'million'], 'hiring_status': ['currently hiring', 'hiring'], 'popular_roles_open': ['engineers'], 'primary_products__services': ['mobile app'], 'investors': ['accel', 'sequoia']}\u001b[0m\u001b[32;1m\u001b[1;3mAction: Numeric Constraint Extraction\n",
      "Action Input: \"List D2C or SaaS companies in Delhi or Hyderabad that raised over â‚¹200 crore, are currently hiring for engineers and PMs, valued above $500 million, offer mobile apps or APIs, and are backed by Sequoia or Accel.\"\u001b[0mâš ï¸ No matching field found nearby. Falling back to 'total_funding_raised_inr'.\n",
      "\u001b[33;1m\u001b[1;3m{'total_funding_raised_inr': {'gte': 500.0}}\u001b[0m\u001b[32;1m\u001b[1;3mAction: filter_composer\n",
      "Action Input: '{\"headquarters_city\": [\"Delhi\", \"Hyderabad\"], \"industry_sector\": [\"D2C\", \"SaaS\"], \"total_funding_raised_inr\": {\"gte\": 200}, \"valuation_usd\": {\"gte\": 500}, \"hiring_status\": \"currently hiring\", \"popular_roles_open\": [\"engineers\", \"PMs\"], \"primary_products__services\": [\"mobile apps\", \"APIs\"], \"investors\": [\"Sequoia\", \"Accel\"]}'\u001b[0m\u001b[38;5;200m\u001b[1;3m{}\u001b[0m\u001b[32;1m\u001b[1;3mFinal Answer: {\"headquarters_city\": [\"Delhi\", \"Hyderabad\"], \"industry_sector\": [\"D2C\", \"SaaS\"], \"total_funding_raised_inr\": {\"gte\": 200}, \"valuation_usd\": {\"gte\": 500}, \"hiring_status\": \"currently hiring\", \"popular_roles_open\": [\"engineers\", \"PMs\"], \"primary_products__services\": [\"mobile apps\", \"APIs\"], \"investors\": [\"Sequoia\", \"Accel\"]}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{\"headquarters_city\": [\"Delhi\", \"Hyderabad\"], \"industry_sector\": [\"D2C\", \"SaaS\"], \"total_funding_raised_inr\": {\"gte\": 200}, \"valuation_usd\": {\"gte\": 500}, \"hiring_status\": \"currently hiring\", \"popular_roles_open\": [\"engineers\", \"PMs\"], \"primary_products__services\": [\"mobile apps\", \"APIs\"], \"investors\": [\"Sequoia\", \"Accel\"]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "executor = AgentExecutor(agent=enhancer_agent, tools=enhancer_tools, verbose=True, handle_parsing_errors=True)\n",
    "\n",
    "query = \"\"\"\n",
    "\n",
    "List D2C or SaaS companies in Delhi or Hyderabad that raised over â‚¹200 crore, are currently hiring for engineers and PMs, valued above $500 million, \n",
    "\n",
    "offer mobile apps or APIs, and are backed by Sequoia or Accel.\"\"\"\n",
    "\n",
    "result = executor.invoke({\"input\": query})\n",
    "print(result[\"output\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
